{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_in_new_tab(driver , element):\n",
    "    ActionChains(driver).scroll_to_element(element)\n",
    "    ActionChains(driver).key_down(Keys.CONTROL).click(element).key_up(Keys.CONTROL).perform()\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "def get_page_source(driver , element):\n",
    "    open_in_new_tab(driver , element)\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        driver.find_element(By.XPATH , '//div[@class = \"top-card-layout__entity-info flex-grow flex-shrink-0 basis-0 babybear:flex-none babybear:w-full babybear:flex-none babybear:w-full\"]')\n",
    "        page = driver.page_source\n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        return page\n",
    "    except:\n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        return get_page_source(driver , element)\n",
    "\n",
    "def get_pages(name , number):\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--incognito\")    # Open in incognito mode\n",
    "\n",
    "    service = Service(r'C:\\Program Files (x86)\\chromedriver.exe')\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    driver.get('https://www.linkedin.com/jobs/search?trk=guest_homepage-basic_guest_nav_menu_jobs&position=1&pageNum=0')\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    search = driver.find_element(By.XPATH , '//input[@id = \"job-search-bar-location\"]')\n",
    "    search.clear()\n",
    "    search.send_keys(name)\n",
    "    search.send_keys(Keys.RETURN)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    jobs = driver.find_elements(By.XPATH , './/section[@class = \"two-pane-serp-page__results-list\"]//ul[@class = \"jobs-search__results-list\"]//li')\n",
    "\n",
    "    while not (len(jobs) >= number):\n",
    "        driver.find_element(By.TAG_NAME , 'body').send_keys(Keys.END)\n",
    "        time.sleep(3)\n",
    "        jobs = driver.find_elements(By.XPATH , './/section[@class = \"two-pane-serp-page__results-list\"]//ul[@class = \"jobs-search__results-list\"]//li')\n",
    "\n",
    "    pages = []\n",
    "\n",
    "    for job in jobs[:number]:\n",
    "        page = get_page_source(driver , job)\n",
    "        pages.append(page)\n",
    "\n",
    "    print(len(pages))\n",
    "    driver.quit()\n",
    "\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Page Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for first 5 jobs in India......\n"
     ]
    }
   ],
   "source": [
    "country_name = input('Enter Country Name : ')\n",
    "number = int(input('How many jobs you want to get extracted : '))\n",
    "print(f'Searching for first {number} jobs in {country_name}......')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "pages = get_pages(country_name , number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_names = []\n",
    "locations = []\n",
    "seniority_levels = []\n",
    "employment_types = []\n",
    "job_functions = []\n",
    "Industries = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    \n",
    "    soup = BeautifulSoup(page ,'html.parser')\n",
    "    try:\n",
    "        intro = soup.find('div' , class_ = 'top-card-layout__entity-info flex-grow flex-shrink-0 basis-0 babybear:flex-none babybear:w-full babybear:flex-none babybear:w-full')\n",
    "    except:\n",
    "        print(f\"Can't find intro...\")\n",
    "        \n",
    "    try:\n",
    "        job_name = intro.find('h1' , class_ = 'top-card-layout__title font-sans text-lg papabear:text-xl font-bold leading-open text-color-text mb-0 topcard__title').text\n",
    "        job_names.append(job_name)\n",
    "    except:\n",
    "        print(f\"Can't find job_name...\")\n",
    "        job_names.append(np.nan)\n",
    "\n",
    "    try:\n",
    "        location = intro.find('span' , class_ = 'topcard__flavor topcard__flavor--bullet')\n",
    "        locations.append(location.text.strip())\n",
    "    except:\n",
    "        print(f\"Can't find location...\")\n",
    "        locations.append(np.nan)\n",
    "\n",
    "    try:\n",
    "        details = soup.find('ul' , class_ = 'description__job-criteria-list').find_all('span' , class_ = 'description__job-criteria-text description__job-criteria-text--criteria')\n",
    "        seniority_levels.append(details[0].text.strip())\n",
    "        employment_types.append(details[1].text.strip())\n",
    "        job_functions.append(details[2].text.strip())\n",
    "        Industries.append(details[3].text.strip())    \n",
    "    except:\n",
    "        print(f\"Can't find details...\")\n",
    "        seniority_levels.append(np.nan)\n",
    "        employment_types.append(np.nan)\n",
    "        job_functions.append(np.nan)\n",
    "        Industries.append(np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrame and Saving into File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Job_Name\" : job_names,\n",
    "    \"Location\" : locations,\n",
    "    \"Seniority_Level\" : seniority_levels,\n",
    "    \"Employment_Type\" : employment_types,\n",
    "    \"Job_Function\" : job_functions,\n",
    "    \"Industry\" : Industries\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.index = pd.RangeIndex(start=1, stop=len(df)+1, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'first_{number}_Jobs_in_{country_name}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
